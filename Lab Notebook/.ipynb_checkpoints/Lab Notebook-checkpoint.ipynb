{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_H, PDF_W = 400, 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "1. [Lyapunov_functions](#Lyapunov_functions)\n",
    "2. [Attractor Networks](#attractor_networks)\n",
    " - [Attractor Networks, 2009](#attractor_networks_mozer)\n",
    " - [Localist Attractor Networks, 2001](#localist_attractor_networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lyapunov Functions <a class=\"anchor\" id=\"Lyapunov_functions\"></a> \n",
    "[To Top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://vanscoy.github.io/docs/papers/taylor2018lyapunov.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105b1c7f0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://vanscoy.github.io/docs/papers/taylor2018lyapunov.pdf\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Minimization Problem, $\\mathcal{P}$\n",
    "Consider a minimization problem:\n",
    "\n",
    "\\begin{equation} \\tag{$\\mathcal{P}$}\n",
    "   \\text{minimize}_{x \\in \\mathbb{R}^d} \\quad f(x),       \n",
    "\\end{equation}\n",
    "where $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "#### Iterative Methods, $\\mathcal{M}$\n",
    "To solve $\\mathcal{P}$, consider methods that iteratively update their estimate of the optimizer using only gradient evaluations. One method for proving convergence of such methods is by finding a *Lyapunov function*.\n",
    "\n",
    "For example, to solve the optimization problem ($\\mathcal{P}$), consider a *first-order iterative fixed-step method* of the form:\n",
    "\n",
    "\\begin{align*} \\tag{$\\mathcal{M}$}\n",
    "y_k &= \\sum_{j=0}^N \\gamma_j x_{k-j} \\\\\n",
    "x_{k+1} &= \\sum_{j=0}^N \\beta_j x_{k-j} - \\alpha \\nabla f(y_k)\n",
    "\\end{align*}\n",
    "\n",
    "for $k \\ge 0$ where $\\alpha, \\beta_j, \\gamma_j$ are the (Fixed) step-sizes and $x_j \\in \\mathbb{R}^d$ for $j = -N, \\ldots, 0$ are the initial conditions. We call the constant $N \\ge 0$ the *degree* of the method.\n",
    "\n",
    "#### Lyapunov Functions\n",
    "*Lyapunov functions* are one of the fundamental tools in control theory that can be used to *verify stability of a dynamical system*. \n",
    "\n",
    "A Lyapunov function can be interpreted as defining an \"energy\" that decreases *geometrically* with each iteration of the method, with an energy of zero corresponding to reaching the optimal solution of $\\mathcal{P}$. The existence of such an energy function provides a straightforward certificate of linear convergence for the iterative method.\n",
    "\n",
    "Consider applying method ($\\mathcal{M}$) to solve problem ($\\mathcal{P}$). Our goal is to find the smallest possible $0 \\le \\rho < 1$ such that $\\{x_k \\}$ converges linearly to the optimizer $x_*$ with rate $\\rho$. A *Lyapunov function* $\\mathcal{V}$ is a continuous function $\\mathcal{V}: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that satisfies the following properties:\n",
    "\n",
    "1. (nonnegative) $\\mathcal{V}(\\xi) \\ge 0$ for all $\\xi$,\n",
    "2. (zero at fixed-point) $\\mathcal{V}(\\xi) = 0$ iff $\\xi = \\xi_*$,\n",
    "3. (radially unbounded) $\\mathcal{V}(\\xi) \\rightarrow \\infty$ as $||\\xi|| \\rightarrow \\infty$,\n",
    "4. (decreasing) $\\mathcal{V}(\\xi_{k+1}) \\le \\rho^2 \\mathcal{V}(\\xi_{k})$ for $k \\le N$,\n",
    "\n",
    "where $\\xi_k := (\\mathbf{x}_k, \\mathbf{g}_k, \\mathbf{f}_k)$ for the *state* of the system at iteration $k$. \n",
    "\n",
    "If we can find such a $\\mathcal{V}$, then it can be used to show that the state converges linearly to the fixed-point from any initial condition, where the rate of convergence depends on both $\\rho$ and the structure of $\\mathcal{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Attractor Networks <a class=\"anchor\" id=\"attractor_networks\"></a> \n",
    "[To Top](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.A. Attractor Networks <a class=\"anchor\" id=\"attractor_networks_mozer\"></a> \n",
    "[To Top](#toc)\n",
    "- Mozer, Michael C. (2009) \n",
    "- http://www.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/Mozer2008.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://www.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/Mozer2008.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105b1c908>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"http://www.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/Mozer2008.pdf\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- An attractor network is a recurrent ANN whose dynamics cause the network state to converge to a fixed point.\n",
    " - I.e., given an input, the dynamics of the network will cause the state to evolve over time to a stable value, away from which the state will not wander.\n",
    " - The states to which the net might evolve are called _attractors_\n",
    "- Attractor dynamics are achieved by many neural network architectures\n",
    " - Hopfield networks \n",
    "   - http://140.116.215.51/course/2012/hopfield.pdf\n",
    "   - http://www.rctn.org/bruno/public/papers/hopfield84.pdf\n",
    " - Harmony networks\n",
    " - Boltzmann Machines\n",
    "   - https://www.cs.toronto.edu/~hinton/absps/cogscibm.pdf\n",
    " - Adaptive resonance networks\n",
    " - Recurrent back prop networks\n",
    "   - A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. Almeida, L.B. (1987)\n",
    "   - Generalization of backpropagation to recurrent and higher order neural networks. Pineda, F.J. (1987)\n",
    "   - https://papers.nips.cc/paper/67-generalization-of-back-propagation-to-recurrent-and-higher-order-neural-networks.pdf\n",
    "\n",
    " \n",
    "- To ensure attractor dynamics, the architecutre **requires** symmetry of connectivity\n",
    " - Symmetry: the conncetion weight from processing unit _A_ to unit _B_ must be the same as the weight from _B_ to _A_.\n",
    " - Given this restriction, the dynamics of the network can be characterized as performing local optimization--minimizing *energy*, or equivalently, maximizing *harmony*.\n",
    " \n",
    "- The **input** to an attractor net can either specify the initial state of the net, or it can provide *biases*--fixed input--to each unit.\n",
    " - **Biases** reshape the landscape such that the best-matching attractor has maximum harmony, and is likely to be found for a wide range of initial network states.\n",
    " \n",
    "- Knowledge of attractor states is distributed over the connectivity pattern of the entire network - as a result, spurious (undesired) and ill-conditioned (e.g., very narrow) attractor basins may exist.\n",
    " - Solution: Localist Attractor Networks (see Zemel & Mozer, 2001)\n",
    " - LANs consist of a set of *state units* and a set of *attractor units* (one per unit). Each attractor unit draws the state toward its attractor, with the attractors closer to the state having a greater influence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.B. Localist Attractor Networks <a class=\"anchor\" id=\"localist_attractor_networks\"></a> \n",
    "[To Top](#toc)\n",
    "- Zemel, Richard S., Mozer, Michael C. (2001)\n",
    "- ftp://128.100.3.31/pub/zemel/Papers/lanNC.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.4161&rep=rep1&type=pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x105b1c0f0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.4161&rep=rep1&type=pdf\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- Attractor networks map an input space (usually continuous) to a sparse output space.\n",
    "- Initial state of the attractor net is determined by the input pattern\n",
    " - Over time the state is drawn to one of a predefined set of states (the attractors)\n",
    "- Often used for **pattern completion**\n",
    "- Pattern completion can be accomplished with other methods -- e.g., nearest-neighbor classification\n",
    " - Attractor networks have benefits over other approaches:\n",
    "   1. Attractors can be characterized by compositional structure - this structure can be encoded implicitly in the attractor network.\n",
    "   2. Attractor networks have some degree of biological plausibility\n",
    "   3. In most formulations, the dynamics can be characterized by gradient descent in an energy landscale, allowing one to partition the output space into attractor basins. In many domains, the energy landscape and the corresponding structure of the attractor basins are key to obtaining desirable behavior. E.g., basins can be sculpted based on the recent history of the network (**priming**) and the arrangement of attractors in the space (**gang effects**).\n",
    "- **Priming** - a network is faster to land at an attractor if it has recently visited the same attractor. Achieved by broadening and deepening attractor basins *as they are visited*. This mechanism allows modelers to account for a ubiquitous property of human behavior: people are faster to perceive a stimulus if they have recently experienced the same or a closely related stimulus.\n",
    "- **Gang Effects** - the strength or pull of an attractor is influenced by other attractors in its neighborhood. \n",
    "\n",
    "- Training attractor networks is notoriously tricky. Why?\n",
    " - Training procedures are **CPU intensive**\n",
    " - **Spurious attractors** form\n",
    " - **Ill-conditioned attractor basins**\n",
    "\n",
    "- No known training procdure exists that can robustly translate an arbitrary specification of an attractor landscape into a set of weights.\n",
    " - Due to the fact that each connection participates in the specification of multiple attractors; **thus, knowledge in the net is distributed over connections**. \n",
    " \n",
    "### Localized Attractor Network\n",
    "#### Benefits\n",
    "1. Trivial procedure for devising architecture given an attractor landscape\n",
    "2. Spurious attractors are eliminated\n",
    "3. An attractor can be primed by adjusted a single parameter of the model\n",
    "4. Achieved gang effects\n",
    "5. Model parameters have a clear mathematical interpretation, which clarifies how the parameters control the qualitative behavior of the model (e.g., the magnitude of gang effects)\n",
    "6. Proofs of convergence and stability\n",
    "\n",
    "#### Structure\n",
    "- Consists of a set of _n_ state units and _m_ attractor units.\n",
    "- Parameters associated with an attractor unit _i_ encode the center in state-space of its attractor basin, denoted $w_i$, and its strength, denoted $\\pi _i$.\n",
    "- The activity of an attractor at time $t$, $q_i(t)$, reflects the normalized distance from its center to the current state, $y(t)$, weighted by its strength:\n",
    "\n",
    "$$ q_i(t) = \\frac{\\pi_i g(y(t), w_i, \\sigma(t))}{\\sum_j \\pi_j g(y(t), w_j, \\sigma(t))}$$\n",
    "\n",
    "$$ g(y, w, \\sigma) = exp(-|y - w|^2 / 2\\sigma^2) $$\n",
    "\n",
    ". . . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamics of Discrete Time, Continuous State Hopfield Networks\n",
    "- Koiran, Pascal (1994)\n",
    "- http://cognet.mit.edu/node/30151"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- Discrete time, discrete state Hopfield network dynamics are driven by an energy function. This allows the length of a limit cycle to be bounded: the parallel iteration has cycles of length 1 or 2 only, and the sequential iteration has only fixed points. These results describe completely the asymptotic behavior of the network, since any trajectory enters a limit cycle after a transient period.\n",
    "\n",
    "- Discrete time, continuous state Hopfield networks are also driven by an energy function. However, a trajectory will generally **not** enter a cycle, so that the discrete-case case argument does not apply and the question of the convergence to a cycle arises.\n",
    "\n",
    "- A lot of math follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Attractors of Discrete-Time Recurrent Neural Networks\n",
    "- Yu, Jiali, et al. (2012)\n",
    "- https://sci-hub.tw/10.1007/s00521-012-0975-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- RNNs can possess more than one and even infinite stable equilibrium points.\n",
    " - These points may be isolated (discrete) or connected (continuous)\n",
    " - Continuous attractors have been used to describe the encoding of continuous stimuli such as eye position, head direction, moving direction, path integrator, cognitive map, and population decoding.\n",
    "- In these networks, RNNs may have a finite number of neurons or infinite number of neurons.\n",
    " - These two categories of RNN differ in their dynamics, from a mathematical point of view.\n",
    " \n",
    "- This paper focuses on continuous attractors with finite number of neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Dynamics in Attractor Networks\n",
    "- Li, Quoqi, et al. (2015)\n",
    "- https://pdfs.semanticscholar.org/840c/a6d4434de3dac8cf13dff6d4bcbbc2164922.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- Propose a new energy function that is nonnegative and attains zero values only at the desired memory patterns.\n",
    "- Following from the contrived energy function, an attractor network is derived. This approach avoids the existence of spurious points (local maxima, saddle points, or other local minima which are undesired memory patters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Denoised Recurrent Neural Networks\n",
    "- Mozer, Michael C., et al (2018)\n",
    "- https://arxiv.org/abs/1805.08394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusions\n",
    "\n",
    "you have a matrix W_{IH} which represents the weights from input to hidden 1.\n",
    "and you have a matrix W_{HI} which represents the weights from hidden 1 to input.\n",
    "\n",
    "symmetry means that W_{IH} = W_{HI}^T . where T is the transpose operator.\n",
    "\n",
    "concerning the atanh: that was to put the input in the logit form so that when it is passed through the tanh you get out what you put in.\n",
    "\n",
    "suppose we have 2 layers, I and H.\n",
    "we also have some external input E which will serve as a bias on I.  In the absence of any additional information, we'd like I=E.  to achieve this:\n",
    "\n",
    "we have an activation function like I = tanh(atanh(E) + b_I + W_{HI} * H)\n",
    "\n",
    "and for H: H=tanh(b_H + W_{IH} * I)\n",
    "\n",
    "the weight constraint you require is that W_{IH} = W_{HI}^T.  for this simple model, let's simply assume no connections between inputs or between hidden.\n",
    "\n",
    "i would implement this with a big matrix which has dimensions (28^2 * n_h) X (28^2 * n_h) \n",
    "where n_h is the number of hidden.  think about the block form of this matrix as we sketched in our meeting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mike Comments\n",
    "- Set bias to 0 when unknown, for inpainting for example.\n",
    "- Korian (1994) Discrete Time - Continuous State \n",
    " - Neurons in parellel\n",
    " \n",
    "- Look into 2000's 2010 Deep RBM papers for datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-14b778121a0b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-14b778121a0b>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    https://colab.research.google.com/notebooks/welcome.ipynb#scrollTo=-Rh3-Vt9Nev9\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- [Asynchronous Dynamics of Continuous Time Neural Networks](http://papers.nips.cc/paper/804-asynchronous-dynamics-of-continuous-time-neural-networks.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
