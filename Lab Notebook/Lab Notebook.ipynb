{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_H, PDF_W = 400, 900"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "1. [Lyapunov_functions](#Lyapunov_functions)\n",
    "2. [Attractor Networks](#attractor_networks_section)<br>\n",
    " A. [Attractor Networks, (Mozer, 2009)](#attractor_networks_mozer) <br>\n",
    " B. [Localist Attractor Networks, (Zemel, Mozer, 2001)](#localist_attractor_networks)<br>\n",
    " C. [Dynamics of Discrete Time, Continuous State Hopfield Networks (Koiran, 1994)](#koiran_1994)<br>\n",
    "3. [Restricted Boltzmann Machines (RBMs)](#RBMs) <br>\n",
    " A. [Overview](#rbm_overview) <br>\n",
    " B. [Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations (Lee, 2009)](#conv_RBM) <br>\n",
    "4. [Recurrent Back Propagation (RBP)](#RBP)<br>\n",
    " A. [Generalization of Backpropagation to Recurrent and Higher Order Neural Networks (Pineda, 1988)](#rbp_pineda) <br>\n",
    " B. [Reviving and Improving Recurrent Back-Propagation (Liao, 2018)](#rbp_liao) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lyapunov Functions <a class=\"anchor\" id=\"Lyapunov_functions\"></a> \n",
    "[To Top](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://vanscoy.github.io/docs/papers/taylor2018lyapunov.pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd02e8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://vanscoy.github.io/docs/papers/taylor2018lyapunov.pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "#### Minimization Problem, $\\mathcal{P}$\n",
    "Consider a minimization problem:\n",
    "\n",
    "\\begin{equation} \\tag{$\\mathcal{P}$}\n",
    "   \\text{minimize}_{x \\in \\mathbb{R}^d} \\quad f(x),       \n",
    "\\end{equation}\n",
    "where $f : \\mathbb{R}^d \\rightarrow \\mathbb{R}$.\n",
    "\n",
    "#### Iterative Methods, $\\mathcal{M}$\n",
    "To solve $\\mathcal{P}$, consider methods that iteratively update their estimate of the optimizer using only gradient evaluations. One method for proving convergence of such methods is by finding a *Lyapunov function*.\n",
    "\n",
    "For example, to solve the optimization problem ($\\mathcal{P}$), consider a *first-order iterative fixed-step method* of the form:\n",
    "\n",
    "\\begin{align*} \\tag{$\\mathcal{M}$}\n",
    "y_k &= \\sum_{j=0}^N \\gamma_j x_{k-j} \\\\\n",
    "x_{k+1} &= \\sum_{j=0}^N \\beta_j x_{k-j} - \\alpha \\nabla f(y_k)\n",
    "\\end{align*}\n",
    "\n",
    "for $k \\ge 0$ where $\\alpha, \\beta_j, \\gamma_j$ are the (Fixed) step-sizes and $x_j \\in \\mathbb{R}^d$ for $j = -N, \\ldots, 0$ are the initial conditions. We call the constant $N \\ge 0$ the *degree* of the method.\n",
    "\n",
    "### Lyapunov Functions\n",
    "*Lyapunov functions* are one of the fundamental tools in control theory that can be used to *verify stability of a dynamical system*. \n",
    "\n",
    "A Lyapunov function can be interpreted as defining an \"energy\" that decreases *geometrically* with each iteration of the method, with an energy of zero corresponding to reaching the optimal solution of $\\mathcal{P}$. The existence of such an energy function provides a straightforward certificate of linear convergence for the iterative method.\n",
    "\n",
    "Consider applying method ($\\mathcal{M}$) to solve problem ($\\mathcal{P}$). Our goal is to find the smallest possible $0 \\le \\rho < 1$ such that $\\{x_k \\}$ converges linearly to the optimizer $x_*$ with rate $\\rho$. A *Lyapunov function* $\\mathcal{V}$ is a continuous function $\\mathcal{V}: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ that satisfies the following properties:\n",
    "\n",
    "1. (nonnegative) $\\mathcal{V}(\\xi) \\ge 0$ for all $\\xi$,\n",
    "2. (zero at fixed-point) $\\mathcal{V}(\\xi) = 0$ iff $\\xi = \\xi_*$,\n",
    "3. (radially unbounded) $\\mathcal{V}(\\xi) \\rightarrow \\infty$ as $||\\xi|| \\rightarrow \\infty$,\n",
    "4. (decreasing) $\\mathcal{V}(\\xi_{k+1}) \\le \\rho^2 \\mathcal{V}(\\xi_{k})$ for $k \\le N$,\n",
    "\n",
    "where $\\xi_k := (\\mathbf{x}_k, \\mathbf{g}_k, \\mathbf{f}_k)$ for the *state* of the system at iteration $k$. \n",
    "\n",
    "If we can find such a $\\mathcal{V}$, then it can be used to show that the state converges linearly to the fixed-point from any initial condition, where the rate of convergence depends on both $\\rho$ and the structure of $\\mathcal{V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Attractor Networks <a class=\"anchor\" id=\"attractor_networks_section\"></a> \n",
    "[To Top](#toc)\n",
    "\n",
    "## Section Table of Contents <a class=\"anchor\" id=\"attractor_networks_section_toc\"></a>\n",
    "A. [Mozer, 2009](#attractor_networks_mozer) <br>\n",
    "B. [Localist Attractor Networks (Zemel, Mozer)](#localist_attractor_networks) <br>\n",
    "C. [Koiran, 1994](#koiran_1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.A. Attractor Networks (Mozer, 2009) <a class=\"anchor\" id=\"attractor_networks_mozer\"></a> \n",
    "[To ToC](#attractor_networks_section_toc)\n",
    "- Mozer, Michael C. (2009) \n",
    "- http://www.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/Mozer2008.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://www.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/Mozer2008.pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd04a8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"http://www.cs.colorado.edu/~mozer/Research/Selected%20Publications/reprints/Mozer2008.pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "- An attractor network is a recurrent ANN whose dynamics cause the network state to converge to a fixed point.\n",
    " - I.e., given an input, the dynamics of the network will cause the state to evolve over time to a stable value, away from which the state will not wander.\n",
    " - The states to which the net might evolve are called _attractors_\n",
    "- Attractor dynamics are achieved by many neural network architectures\n",
    " - Hopfield networks \n",
    "   - http://140.116.215.51/course/2012/hopfield.pdf\n",
    "   - http://www.rctn.org/bruno/public/papers/hopfield84.pdf\n",
    " - Harmony networks\n",
    " - Boltzmann Machines\n",
    "   - https://www.cs.toronto.edu/~hinton/absps/cogscibm.pdf\n",
    " - Adaptive resonance networks\n",
    " - Recurrent back prop networks\n",
    "   - A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. Almeida, L.B. (1987)\n",
    "   - Generalization of backpropagation to recurrent and higher order neural networks. Pineda, F.J. (1987)\n",
    "   - https://papers.nips.cc/paper/67-generalization-of-back-propagation-to-recurrent-and-higher-order-neural-networks.pdf\n",
    "\n",
    " \n",
    "- To ensure attractor dynamics, the architecutre **requires** symmetry of connectivity\n",
    " - Symmetry: the conncetion weight from processing unit _A_ to unit _B_ must be the same as the weight from _B_ to _A_.\n",
    " - Given this restriction, the dynamics of the network can be characterized as performing local optimization--minimizing *energy*, or equivalently, maximizing *harmony*.\n",
    " \n",
    "- The **input** to an attractor net can either specify the initial state of the net, or it can provide *biases*--fixed input--to each unit.\n",
    " - **Biases** reshape the landscape such that the best-matching attractor has maximum harmony, and is likely to be found for a wide range of initial network states.\n",
    " \n",
    "- Knowledge of attractor states is distributed over the connectivity pattern of the entire network - as a result, spurious (undesired) and ill-conditioned (e.g., very narrow) attractor basins may exist.\n",
    " - Solution: Localist Attractor Networks (see Zemel & Mozer, 2001)\n",
    " - LANs consist of a set of *state units* and a set of *attractor units* (one per unit). Each attractor unit draws the state toward its attractor, with the attractors closer to the state having a greater influence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.B. Localist Attractor Networks (Zemel, Mozer, 2001)<a class=\"anchor\" id=\"localist_attractor_networks\"></a> \n",
    "[To ToC](#attractor_networks_section_toc)\n",
    "- Zemel, Richard S., Mozer, Michael C. (2001)\n",
    "- ftp://128.100.3.31/pub/zemel/Papers/lanNC.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.4161&rep=rep1&type=pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd06a0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.33.4161&rep=rep1&type=pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "- Attractor networks map an input space (usually continuous) to a sparse output space.\n",
    "- Initial state of the attractor net is determined by the input pattern\n",
    " - Over time the state is drawn to one of a predefined set of states (the attractors)\n",
    "- Often used for **pattern completion**\n",
    "- Pattern completion can be accomplished with other methods -- e.g., nearest-neighbor classification\n",
    " - Attractor networks have benefits over other approaches:\n",
    "   1. Attractors can be characterized by compositional structure - this structure can be encoded implicitly in the attractor network.\n",
    "   2. Attractor networks have some degree of biological plausibility\n",
    "   3. In most formulations, the dynamics can be characterized by gradient descent in an energy landscale, allowing one to partition the output space into attractor basins. In many domains, the energy landscape and the corresponding structure of the attractor basins are key to obtaining desirable behavior. E.g., basins can be sculpted based on the recent history of the network (**priming**) and the arrangement of attractors in the space (**gang effects**).\n",
    "- **Priming** - a network is faster to land at an attractor if it has recently visited the same attractor. Achieved by broadening and deepening attractor basins *as they are visited*. This mechanism allows modelers to account for a ubiquitous property of human behavior: people are faster to perceive a stimulus if they have recently experienced the same or a closely related stimulus.\n",
    "- **Gang Effects** - the strength or pull of an attractor is influenced by other attractors in its neighborhood. \n",
    "\n",
    "- Training attractor networks is notoriously tricky. Why?\n",
    " - Training procedures are **CPU intensive**\n",
    " - **Spurious attractors** form\n",
    " - **Ill-conditioned attractor basins**\n",
    "\n",
    "- No known training procdure exists that can robustly translate an arbitrary specification of an attractor landscape into a set of weights.\n",
    " - Due to the fact that each connection participates in the specification of multiple attractors; **thus, knowledge in the net is distributed over connections**. \n",
    " \n",
    "### Localized Attractor Network\n",
    "#### Benefits\n",
    "1. Trivial procedure for devising architecture given an attractor landscape\n",
    "2. Spurious attractors are eliminated\n",
    "3. An attractor can be primed by adjusted a single parameter of the model\n",
    "4. Achieved gang effects\n",
    "5. Model parameters have a clear mathematical interpretation, which clarifies how the parameters control the qualitative behavior of the model (e.g., the magnitude of gang effects)\n",
    "6. Proofs of convergence and stability\n",
    "\n",
    "#### Structure\n",
    "- Consists of a set of _n_ state units and _m_ attractor units.\n",
    "- Parameters associated with an attractor unit _i_ encode the center in state-space of its attractor basin, denoted $w_i$, and its strength, denoted $\\pi _i$.\n",
    "- The activity of an attractor at time $t$, $q_i(t)$, reflects the normalized distance from its center to the current state, $y(t)$, weighted by its strength:\n",
    "\n",
    "$$ q_i(t) = \\frac{\\pi_i g(y(t), w_i, \\sigma(t))}{\\sum_j \\pi_j g(y(t), w_j, \\sigma(t))}$$\n",
    "\n",
    "$$ g(y, w, \\sigma) = exp(-|y - w|^2 / 2\\sigma^2) $$\n",
    "\n",
    ". . . \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.C. Dynamics of Discrete Time, Continuous State Hopfield Networks (Koiran, 1994) <a class=\"anchor\" id=\"koiran_1994\"></a> \n",
    "[To ToC](#attractor_networks_section_toc)\n",
    "\n",
    "- Koiran, Pascal (1994)\n",
    "- https://sci-hub.tw/10.1162/neco.1994.6.3.459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://dabamirror.sci-hub.tw/4428/cffc2674381d0fdb7b2fbecdd57498f5/koiran1994.pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd0278>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://dabamirror.sci-hub.tw/4428/cffc2674381d0fdb7b2fbecdd57498f5/koiran1994.pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Points\n",
    "**1**. Any trajectory converges to a fixed point for the *sequential iteration mode*\n",
    "\n",
    "**2**. Any trajectory converges to a cycle of length 2 or a fixed point for the *parallel* iteration mode.\n",
    "\n",
    "More specifically,\n",
    "- Discrete time, discrete state Hopfield network dynamics are driven by an energy function. This allows the length of a limit cycle to be bounded: the parallel iteration has cycles of length 1 or 2 only, and the sequential iteration has only fixed points. These results describe completely the asymptotic behavior of the network, since any trajectory enters a limit cycle after a transient period.\n",
    "\n",
    "- Discrete time, continuous state Hopfield networks are also driven by an energy function. However, a trajectory will generally **not** enter a cycle, so that the discrete-case case argument does not apply and the question of the convergence to a cycle arises.\n",
    "\n",
    "The key contribution of this paper is that is provides mathematical proofs for the statements above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 Math: Preliminaries\n",
    "\n",
    "#### State Update\n",
    "Consider a network of $n$ interconnected neurons, whose states $x_1, \\ldots, x_n$ belong to $[-1, 1]$. The transition function of neuron $i$ is $x_i \\mapsto f(A_i)$, where $A_i$ is the activation of neuron $i$, defined by:\n",
    "\n",
    "$$ A_i (x) = \\sum_{j=1}^n w_{ij}x_j - b_i$$\n",
    "\n",
    "where $b_i$ is the threshold of neuron $i$, and $w_{ij}$ is the weight of the connection between neurons $i and j$. Note, $b = (b_i)_{i \\le i \\le n}$ is the vector of thresholds. \n",
    "\n",
    "**N.B.** The matrix of weights $W = (w_{ij})$ is assumed to be **symmetric**, with a *nonnegative diagonal*. \n",
    "\n",
    "#### Activation Function, $f$\n",
    "$f$ is continuous, strictly increasing on an interval $[\\alpha, \\beta] \\quad (\\alpha < \\beta \\text{ and, possibly, } \\alpha = -\\infty \\text{ or } \\beta = +\\infty)$, and constant outside\n",
    "\\begin{align*}\n",
    "\\forall x \\le \\alpha, f(x) &= -1 \\\\\n",
    "\\forall x \\ge \\beta, f(x) &= +1 \\\\\n",
    "\\end{align*}\n",
    "If $a = -\\infty$ or $\\beta = +\\infty$, we ask that $\\lim_{x\\rightarrow \\pm \\infty} f(x) = \\pm 1$.\n",
    "\n",
    "$f$ may be piecewise $C^1$. In short, \n",
    "> A continuously differentiably function $f(x)$ is a function whose derivative function $f^\\prime(x)$ is also continuous at the point in question.\n",
    "\n",
    "- See [C^k notation](http://mathworld.wolfram.com/C-kFunction.html) or [smoothness](https://en.wikipedia.org/wiki/Smoothness) for more detailed discussion.\n",
    "\n",
    "#### Parallel and Sequential Iteration\n",
    "In *parallel iteration*, all neurons change state simultaneously: for $t \\in N$ and $1 \\le i \\le n$:\n",
    "\n",
    "$$ x_i(t+1) = f(A_i(t))$$\n",
    "\n",
    "In *sequential iteration*, neurons are updated in increasing order:\n",
    "$$ x_i(t+1/n) = f\\left(A_i \\left( \\frac{t+(i-1)}{n} \\right) \\right)$$\n",
    "\n",
    "Let $F$ be the function associated to a given iteration mode: \n",
    "- $F = P$ (parallel mode)\n",
    "- $F = S$ (sequential mode)\n",
    "\n",
    "**N.B.** Assuming a specific update order for sequential iterations is in fact not necessary. It is sufficient to update each neuron an infinite number of times. Note that these iteration modes have the same fixed points. \n",
    "\n",
    "#### Cycle Definition\n",
    "A cycle of length $T$ is a sequence $(y^0, \\ldots, y^{T-1})$ of distinct states such that $F(y^i) = y^{(i+1) \\text{ mod } T}$.\n",
    "\n",
    "We say that a sequence $(x(t))$ of iterates converges to this cycle if for any $i$ such that $0 \\le i \\le T - 1, \\lim_{t \\rightarrow +\\infty} x(Tt+i) = y^i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Math: Sequential Mode\n",
    "\n",
    "The existence of a [Lyapunov function](#Lyapunov_functions) for the sequential iteration and its consequence on the length of cycles are stated in Theorem 1 and Corollary 1.\n",
    "\n",
    "**N.B.** Theorem 2 and 4 are key results from this paper.\n",
    "\n",
    "---\n",
    "\n",
    "**Theorem 1**\n",
    ">Let $E$ be defined by <br>\n",
    "$$ E(x) = -x^TWx/2 + b^Tx + \\sum_{i=1}^n \\int_0^{x_i} f^{-1}(\\xi) d\\xi$$ <br>\n",
    "E is a *Lyapunov function* of the sequential iteration. I.e., <br>\n",
    "if $x(t+1/n) \\neq x(t), E(x(t+1/n)) < E(x(t))$.\n",
    "\n",
    "**Corollary 1**\n",
    ">Any cycle of the sequential iteration is a fixed point. <br>\n",
    "\n",
    "This is a standard consequence of the existence of a Lyapunov function. \n",
    "\n",
    "---\n",
    "\n",
    "**Theorem 2**\n",
    ">Assume the hypothesis (H) is true: the network has a finite number of fixed points. Then under this hypothesis, the sequential iteration converges to a fixed point from any starting point $x^0 \\in [-1, 1]^n$.\n",
    "\n",
    "- This theorem is a general result on dynamic systems driven by a Lyapunov function: the specific form of the iterated function or of the energy function is not important. \n",
    "\n",
    "---\n",
    "\n",
    "**Theorem 4**\n",
    ">When $f$ is piecewise $C^1$, the network has a finite number of fixed points for $(W, b)$ in an open dense set.\n",
    "\n",
    "Definitions:\n",
    "- **Closed Set** ([Springer](https://link.springer.com/chapter/10.1007/978-1-84628-627-8_4)) : a subset of a metric space that includes all of its boundary\n",
    "- **Open Set** ([Springer](https://link.springer.com/chapter/10.1007/978-1-84628-627-8_4)): a subset that contains no point of its boundary\n",
    "\n",
    "  - E.g., (-1, 1) is open; [-1, 1] is closed. \n",
    "\n",
    "- **Dense Set** ([wiki](https://en.wikipedia.org/wiki/Dense_set)):\n",
    "> a subset $A$ of a topological space $X$ is called dense (in $X$) if every point $x$ in $X$ either belongs to $A$ or is a limit point of $A$; that is, the closure of $A$ is constituting the whole set $X$.\n",
    "\n",
    "- **Compact Space** ([wiki](https://en.wikipedia.org/wiki/Compact_space))\n",
    ">In mathematics, and more specifically in general topology, compactness is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other). Examples include a closed interval, a rectangle, or a finite set of points. This notion is defined for more general topological spaces than Euclidean space in various ways.<br><br>\n",
    "One such generalization is that a topological space is sequentially compact if every infinite sequence of points sampled from the space has an infinite subsequence that converges to some point of the space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 Math: Parallel Mode\n",
    "**Corollary 2**\n",
    ">Any cycle of the parallel iteration is of length 1 or 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Restricted Boltzman Machines (RBMs) <a class=\"anchor\" id=\"RBMs\"></a> \n",
    "[To Top](#toc)\n",
    "\n",
    "## Section Table of Contents <a class=\"anchor\" id=\"rbm_section_toc\"></a>\n",
    "A. [Overview](#rbm_overview) <br>\n",
    "B. [Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations](#conv_RBM) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.A. Overview <a class=\"anchor\" id=\"rbm_overview\"></a> \n",
    "[To ToC](#rbm_section_toc)\n",
    "- http://deeplearning.net/tutorial/rbm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://deeplearning.net/tutorial/rbm.html#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd0940>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"http://deeplearning.net/tutorial/rbm.html#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.B. Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations <a class=\"anchor\" id=\"conv_RBM\"></a> \n",
    "[To ToC](#rbm_section_toc)\n",
    "- Lee, et al. (2009)\n",
    "- https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd09e8>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://ai.stanford.edu/~ang/papers/icml09-ConvolutionalDeepBeliefNetworks.pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Recurrent Back Propagation (RBP) <a class=\"anchor\" id=\"RBP\"></a> \n",
    "[To Top](#toc)\n",
    "\n",
    "## Section Table of Contents <a class=\"anchor\" id=\"rbp_section_toc\"></a>\n",
    "A. [Generalization of Backpropagation to Recurrent and Higher Order Neural Networks](#rbp_pineda) <br>\n",
    "B. [Reviving and Improving Recurrent Back-Propagation](#rbp_liao) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.A. Generalization of Backpropagation to Recurrent and Higher Order Neural Networks (Pineda, 1988) <a class=\"anchor\" id=\"rbp_pineda\"></a> \n",
    "[To ToC](#rbp_section_toc)\n",
    "- FJ Pineda (1988)\n",
    "- https://papers.nips.cc/paper/67-generalization-of-back-propagation-to-recurrent-and-higher-order-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"https://papers.nips.cc/paper/67-generalization-of-back-propagation-to-recurrent-and-higher-order-neural-networks.pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd0ba8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://papers.nips.cc/paper/67-generalization-of-back-propagation-to-recurrent-and-higher-order-neural-networks.pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.B  Reviving and Improving Recurrent Back-Propagation (Liao, 2018) <a class=\"anchor\" id=\"rbp_liao\"></a> \n",
    "[To ToC](#rbp_section_toc)\n",
    "- Liao, et al. (2018)\n",
    "- http://xaqlab.com/wp-content/uploads/2018/07/RecurrentBackprop_IMCL.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"400\"\n",
       "            src=\"http://xaqlab.com/wp-content/uploads/2018/07/RecurrentBackprop_IMCL.pdf#view=FitH\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x104fd0d30>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"http://xaqlab.com/wp-content/uploads/2018/07/RecurrentBackprop_IMCL.pdf#view=FitH\", width=PDF_W, height=PDF_H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Attractors of Discrete-Time Recurrent Neural Networks\n",
    "- Yu, Jiali, et al. (2012)\n",
    "- https://sci-hub.tw/10.1007/s00521-012-0975-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- RNNs can possess more than one and even infinite stable equilibrium points.\n",
    " - These points may be isolated (discrete) or connected (continuous)\n",
    " - Continuous attractors have been used to describe the encoding of continuous stimuli such as eye position, head direction, moving direction, path integrator, cognitive map, and population decoding.\n",
    "- In these networks, RNNs may have a finite number of neurons or infinite number of neurons.\n",
    " - These two categories of RNN differ in their dynamics, from a mathematical point of view.\n",
    " \n",
    "- This paper focuses on continuous attractors with finite number of neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Dynamics in Attractor Networks\n",
    "- Li, Quoqi, et al. (2015)\n",
    "- https://pdfs.semanticscholar.org/840c/a6d4434de3dac8cf13dff6d4bcbbc2164922.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "- Propose a new energy function that is nonnegative and attains zero values only at the desired memory patterns.\n",
    "- Following from the contrived energy function, an attractor network is derived. This approach avoids the existence of spurious points (local maxima, saddle points, or other local minima which are undesired memory patters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State-Denoised Recurrent Neural Networks\n",
    "- Mozer, Michael C., et al (2018)\n",
    "- https://arxiv.org/abs/1805.08394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
